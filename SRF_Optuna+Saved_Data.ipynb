{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.model_selection as skm\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import (DecisionTreeClassifier as DTC, DecisionTreeRegressor as DTR, plot_tree, export_text)\n",
    "from sklearn.metrics import (accuracy_score, log_loss)\n",
    "from sklearn.ensemble import (RandomForestRegressor as RF, GradientBoostingRegressor as GBR)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import loguniform\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import csv\n",
    "from matplotlib.pyplot import subplots\n",
    "import sklearn.model_selection as skm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import (DecisionTreeClassifier as DTC, DecisionTreeRegressor as DTR, plot_tree, export_text)\n",
    "from sklearn.metrics import (accuracy_score, log_loss)\n",
    "from sklearn.ensemble import (RandomForestRegressor as RF, GradientBoostingRegressor as GBR)\n",
    "from scipy.stats import loguniform, randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from scipy.stats import randint, uniform, loguniform\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
    "import smogn\n",
    "import os\n",
    "from sklearn.svm import SVR\n",
    "import concurrent.futures\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_balanced = pd.read_csv('Resampling_output_data/unbalanced.csv')\n",
    "X_test = pd.read_csv('Resampling_output_data/X_test_unbalanced.csv')\n",
    "y_test = pd.read_csv('Resampling_output_data/y_test_unbalanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(y_test).value_counts())\n",
    "print(pd.DataFrame(y_test).value_counts(normalize=True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_balanced_train = recovery_balanced.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Duration = recovery_balanced_train['Duration_int']\n",
    "Recovery_rate = recovery_balanced_train['Actual_Recovery_Rate_label']\n",
    "X_train = recovery_balanced_train.drop(columns = ['Duration_int','Actual_Recovery_Rate_label','Expected_Recoveries','Actual_Recoveries_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, columns = X_train.columns)\n",
    "X_train['Duration'] = Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = Recovery_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Duration = X_test['Duration_int']\n",
    "X_test = X_test.drop(columns = ['Unnamed: 0','Duration_int','Expected_Recoveries','Actual_Recovery_Rate_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.get_dummies(X_test, columns = X_test.columns)\n",
    "X_test['Duration'] = Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.align(X_test, join='outer', axis=1, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = y_test.drop(columns = 'Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "__ = pd.concat([X_train, y_train], axis = 1)\n",
    "__.reset_index(inplace=True)\n",
    "_ = __.sample(frac = 1)\n",
    "_.reset_index(inplace=True)\n",
    "y_train = _['Actual_Recovery_Rate_label']\n",
    "X_train = _.drop(columns = ['Actual_Recovery_Rate_label','level_0','index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(y_train),max(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.terminator.callback import TerminatorCallback\n",
    "from optuna.terminator import report_cross_validation_scores\n",
    "from optuna.trial import TrialState\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_db_path = \"sqlite:////Users/mingyanggao/Desktop/SRF/my_optuna_study.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "optuna-dashboard sqlite:////Users/mingyanggao/Desktop/SRF/my_optuna_study.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## Past Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "MSE from the DTR of the provided paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import root_mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_DTR_pre = DTR(max_depth = 15, min_samples_split = 110, ccp_alpha = 0.0000001)\n",
    "model_DTR_pre.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure y_test is a 1D numpy array of the same length as X_test\n",
    "if isinstance(y_test, pd.DataFrame) or isinstance(y_test, pd.Series):\n",
    "    y_test = y_test.values.flatten()\n",
    "if hasattr(y_test, 'shape') and len(y_test.shape) > 1:\n",
    "    y_test = y_test.flatten()\n",
    "if hasattr(y_test, 'reset_index'):\n",
    "    y_test = pd.Series(y_test).reset_index(drop=True).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_DTR_pre.predict(X_test)\n",
    "root_mean_squared_error(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# Summary and Nonparametric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN_score = test_loss.item() \n",
    "CatBoost_score = 3.3276566434721118\n",
    "XGBoost_score = 3.335625247106174 \n",
    "Pre_score = 3.3285080299643295"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "do the catboost extraction of pre-coded data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = 5, shuffle = True, random_state = 1)\n",
    "cv_previous = []\n",
    "cv_cat = []\n",
    "cv_XGBoost = []\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "for train_idx, test_idx in kf.split(X_test, y_test):\n",
    "    X_test_batch = X_test.iloc[test_idx]\n",
    "    y_test_batch = y_test[test_idx]\n",
    "\n",
    "    a = root_mean_squared_error(model_DTR_pre.predict(X_test_batch),y_test_batch)\n",
    "    cv_previous.append(a)\n",
    "   \n",
    "\n",
    "for train_idx, test_idx in kf.split(X_test_, y_test):\n",
    "    X_test_batch = X_test_.iloc[test_idx]\n",
    "    y_test_batch = y_test[test_idx]\n",
    "\n",
    "    test_pool = Pool(data = X_test_batch, cat_features = ['AgeBand_cat', 'Disability_cat', 'OwnOcc_cat', 'STD_cat', 'Gender_bin_cat', 'Benefit_amount_cat'])\n",
    "\n",
    "    dtest = xgb.DMatrix(\n",
    "      X_test_batch, \n",
    "      enable_categorical=True)\n",
    "\n",
    "    #catboost\n",
    "    pred_cat = trial_model.predict(test_pool)\n",
    "    rmse_cat = root_mean_squared_error(y_test_batch, pred_cat)\n",
    "\n",
    "    #xgboost\n",
    "    pred_xgb = bst.predict(dtest)\n",
    "    rmse_xgboost = root_mean_squared_error(y_test_batch, pred_xgb)\n",
    "\n",
    "    cv_cat.append(rmse_cat)\n",
    "    cv_XGBoost.append(rmse_xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#KFold CV with NN\n",
    "\n",
    "cv_NN = list()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "_ = torch.cat([X_train_tensor, y_train_tensor], dim=1)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(_)):\n",
    "    X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "    \n",
    "    a = X_train.shape[1]\n",
    "    X_tr_numeric = X_tr.astype(np.float32)\n",
    "    X_val_numeric = X_val.astype(np.float32)\n",
    "\n",
    "    X_tr_tensor = torch.tensor(X_tr_numeric.values, dtype=torch.float32)\n",
    "    y_tr_tensor = torch.tensor(y_tr.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    X_val_tensor = torch.tensor(X_val_numeric.values, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val.to_numpy(), dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "    X_tr_tensor = X_tr_tensor.to(DEVICE)\n",
    "    y_tr_tensor = y_tr_tensor.to(DEVICE)\n",
    "    X_val_tensor = X_val_tensor.to(DEVICE)\n",
    "    y_val_tensor = y_val_tensor.to(DEVICE)\n",
    "\n",
    "    for epoch in range(50):\n",
    "        NN.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = NN(X_tr_tensor)\n",
    "        loss = criterion(output, y_tr_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    NN.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = NN(X_val_tensor)\n",
    "        val_loss = criterion(predictions, y_val_tensor)\n",
    "        cv_NN.append(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_NN_list = [13.3711, 16.2190, 15.1451, 13.6903, 12.8620]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kruskal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat, p_value = kruskal(cv_previous, cv_XGBoost, cv_cat)\n",
    "print(f\"Kruskal-Wallis H-statistic: {stat}, p-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = pd.DataFrame({'Models' : ['Pre','XGBoost','cat'],\n",
    "              'Score':[Pre_score,XGBoost_score,CatBoost_score]})\n",
    "sns.barplot(x='Models',y='Score',data = _)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'previous':cv_previous,\n",
    "              'boost':cv_boost,\n",
    "              'NN':cv_NN_list})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikit_posthocs as sp\n",
    "\n",
    "# Combine data into a DataFrame\n",
    "data = list(cv_previous) + list(cv_boost) + cv_NN_list\n",
    "groups = ['previous']*len(cv_previous) + ['boost']*len(cv_boost) + ['NN']*len(cv_NN_list) \n",
    "df_2 = pd.DataFrame({'group': groups, 'value': data})\n",
    "\n",
    "# Perform Dunn's test with Bonferroni correction\n",
    "posthoc = sp.posthoc_dunn(df_2, val_col='value', group_col='group', p_adjust='bonferroni')\n",
    "print(posthoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "stat, p = mannwhitneyu(list(cv_previous), list(cv_cat), alternative='two-sided')\n",
    "print(f\"Previous rMSEs: {list(cv_previous)}\")\n",
    "print(f\"XGBoost rMSEs: {list(cv_cat)}\")\n",
    "print(f\"Mann-Whitney U statistic: {stat}, p-value: {p}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"Difference in MSE is statistically significant.\")\n",
    "else:\n",
    "    print(\"No significant difference in MSE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "        max_seconds = 60\n",
    "        start_time = time.time()\n",
    "\n",
    "        svr_kernel = trial.suggest_categorical('kernel', ['rbf', 'poly'])\n",
    "        svr_shrink = trial.suggest_categorical('shrinking', [True, False])\n",
    "        svr_C = trial.suggest_float('svr_rbf_C', 1e-3, 1e3, log=True)\n",
    "        svr_epsilon = trial.suggest_float('epsilon', 0.1, 1, step=0.1)\n",
    "        regressor_obj = SVR(kernel=svr_kernel, shrinking=svr_shrink, C=svr_C, epsilon=svr_epsilon, gamma='auto')\n",
    "\n",
    "        #sample run\n",
    "        #small_X, _, small_y, _ = train_test_split(X_train, y_train, test_size=0.5)\n",
    "        #start_fit = time.time()\n",
    "        #regressor_obj.fit(small_X, small_y)\n",
    "        #fit_duration = time.time() - start_fit\n",
    "        #if fit_duration > max_seconds:\n",
    "        #    raise optuna.TrialPruned()\n",
    "\n",
    "        scores = skm.cross_val_score(regressor_obj, X_train, y_train, n_jobs=-1, cv=3, scoring='neg_mean_squared_error')\n",
    "        actual_mse_scores = [-s for s in scores]\n",
    "        mean_mse = np.mean(actual_mse_scores)\n",
    "        report_cross_validation_scores(trial, scores.tolist())\n",
    "        return (mean_mse)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(direction='minimize', \n",
    "                            storage=optuna_db_path, \n",
    "                            study_name='SVR_5%', \n",
    "                            load_if_exists=True)\n",
    "    study.optimize(objective,\n",
    "                   n_trials= 50)\n",
    "\n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "##  Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "#### pre-coded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_balanced_ = pd.read_csv('Resampling_output_data/unbalanced.csv')\n",
    "X_test_ = pd.read_csv('Resampling_output_data/X_test_unbalanced.csv')\n",
    "y_test_ = pd.read_csv('Resampling_output_data/y_test_unbalanced.csv')\n",
    "\n",
    "recovery_balanced_train_ = recovery_balanced.drop(columns = ['Unnamed: 0'])\n",
    "X_train_ = recovery_balanced_train.drop(columns = ['Actual_Recovery_Rate_label','Expected_Recoveries','Actual_Recoveries_label'])\n",
    "\n",
    "y_train_ = Recovery_rate\n",
    "\n",
    "\n",
    "X_test_ = X_test_.drop(columns = ['Unnamed: 0','Expected_Recoveries','Actual_Recovery_Rate_label'])\n",
    "y_test_ = y_test_.drop(columns = 'Unnamed: 0')\n",
    "y_test_ = np.array(y_test_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_alt, X_val, y_train_alt, y_val = train_test_split(X_train_,y_train_,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pool = Pool(\n",
    "    data = X_train_alt, \n",
    "    label = y_train_alt,\n",
    "    cat_features = ['AgeBand_cat', 'Disability_cat', 'OwnOcc_cat',\n",
    "       'STD_cat', 'Gender_bin_cat', 'Benefit_amount_cat'])\n",
    "\n",
    "valid_pool = Pool(\n",
    "    data = X_val, \n",
    "    label = y_val,\n",
    "    cat_features = ['AgeBand_cat', 'Disability_cat', 'OwnOcc_cat',\n",
    "       'STD_cat', 'Gender_bin_cat', 'Benefit_amount_cat'])\n",
    "       \n",
    "test_pool = Pool(\n",
    "    data = X_test_, \n",
    "    cat_features = ['AgeBand_cat', 'Disability_cat', 'OwnOcc_cat',\n",
    "    'STD_cat', 'Gender_bin_cat', 'Benefit_amount_cat'])\n",
    "\n",
    "report_pool = Pool(\n",
    "    data = X_train_,\n",
    "     label = y_train_,\n",
    "    cat_features = ['AgeBand_cat', 'Disability_cat', 'OwnOcc_cat',\n",
    "    'STD_cat', 'Gender_bin_cat', 'Benefit_amount_cat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "#### Mode Using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "        param = {\n",
    "        \"thread_count\": -1,   \n",
    "        'grow_policy':'Lossguide',\n",
    "        'iterations': 100000,\n",
    "        'learning_rate': trial.suggest_float(\"learning_rate\", 0.1, 1),\n",
    "        \"depth\" : trial.suggest_int(\"depth\", 5,20),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 25, 100),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1),\n",
    "        \"colsample_bylevel\": trial.suggest_float(\"colsample_bylevel\", 0.5, 1),\n",
    "        #\"bootstrap_type\": trial.suggest_categorical(\"bootstrap_type\", [\"Bayesian\", \"Bernoulli\"]),\n",
    "        \"max_leaves\": trial.suggest_int(\"max_leaves\", 20, 100),\n",
    "        'od_type':\"Iter\",\n",
    "        'od_wait':20\n",
    "        }\n",
    "\n",
    "        trial_model  = CatBoostRegressor(**param).fit(\n",
    "            train_pool,\n",
    "            eval_set = valid_pool\n",
    "        )\n",
    "        \n",
    "        pred = trial_model.predict(test_pool)\n",
    "        rmse = root_mean_squared_error(y_test, pred)\n",
    "        \n",
    "        trial.set_user_attr(\"best_iter\", trial_model.get_best_iteration())\n",
    "        return rmse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #sampler = TPESampler()\n",
    "    study = optuna.create_study(#sampler = sampler,\n",
    "                                storage=optuna_db_path,\n",
    "                                study_name='Final_code_CatBoost_Optuna',\n",
    "                                load_if_exists=True,\n",
    "                                direction=\"minimize\")\n",
    "\n",
    "    study.optimize(objective, n_trials=20000)\n",
    "\n",
    "    print(\"Number of finished5rt6 trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Model Without Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_model  = CatBoostRegressor(\n",
    "            od_wait = 50,\n",
    "            use_best_model=True\n",
    ").fit(\n",
    "            train_pool,\n",
    "            eval_set = valid_pool\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trial_model.predict(test_pool)\n",
    "rmse = root_mean_squared_error(y_test, pred)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_duration = X_train_['Duration_int']\n",
    "X_train = X_train_.drop(columns = ['Duration_int'])\n",
    "X_test_duration = X_test_['Duration_int']\n",
    "X_test  = X_test_.drop(columns = ['Duration_int'])\n",
    "\n",
    "X_train_.columns  = X_train_.columns.astype(str)\n",
    "X_test_.columns   = X_test_.columns.astype(str)\n",
    "X_train_.columns  = X_train_.columns.str.replace(r'[\\$ ,)-]', '_', regex=True)\n",
    "X_test_.columns   = X_test_.columns.str.replace(r'[\\$ ,)-]', '_', regex=True)\n",
    "\n",
    "X_train_['Duration_int'] =  X_train_duration\n",
    "X_test_['Duration_int']  =  X_test_duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in ['AgeBand_cat', 'Disability_cat', 'OwnOcc_cat', 'STD_cat',\n",
    "       'Gender_bin_cat', 'Benefit_amount_cat']:\n",
    "       X_train_[x] = X_train_[x].astype('category')\n",
    "       X_test_[x] = X_test_[x].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_alt, X_val, y_train_alt, y_val = train_test_split(X_train_,y_train_,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(\n",
    "    X_train_alt, \n",
    "    label = y_train_alt,\n",
    "    enable_categorical=True)\n",
    "\n",
    "dvalid = xgb.DMatrix(\n",
    "    X_val, \n",
    "    label = y_val,\n",
    "    enable_categorical=True)\n",
    "       \n",
    "dtest = xgb.DMatrix(\n",
    "    X_test_, \n",
    "    enable_categorical=True)\n",
    "\n",
    "dreport = xgb.DMatrix(\n",
    "    X_train_,\n",
    "    label = y_train,\n",
    "    enable_categorical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"booster\": \"gbtree\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"max_depth\": 6,\n",
    "    \"min_child_weight\": 1,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"gamma\": 5,\n",
    "    \"lambda\": 1,\n",
    "    \"alpha\": 1,\n",
    "    \"tree_method\": \"hist\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain.num_row(), dvalid.num_row(), dtest.num_row()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    param = {\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"verbosity\": 1,\n",
    "        \"eval_metric\": 'rmse',\n",
    "        \"learning_rate\": trial.suggest_float('learning_rate',0.01,0.04),\n",
    "        \"max_depth\": trial.suggest_int('max_depth',1,5),\n",
    "        \"subsample\": trial.suggest_float('subsample',0.5,1),\n",
    "        \"gamma\":  trial.suggest_float('gamma',1,20),\n",
    "        \"lambda\": trial.suggest_float('gamma',1,20),\n",
    "        \"alpha\": trial.suggest_float('gamma',1,20),\n",
    "        \"tree_method\": \"hist\"\n",
    "    }\n",
    "\n",
    "    model = xgb.train(\n",
    "    param,\n",
    "    dtrain = dtrain,\n",
    "    num_boost_round=5000,\n",
    "    evals=[(dvalid, 'valid')],\n",
    "    early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    bst = xgb.train(\n",
    "    param,\n",
    "    dtrain = dtrain,\n",
    "    num_boost_round= model.best_iteration\n",
    "    #evals=[(dvalid, 'valid')],\n",
    "    #early_stopping_rounds=50\n",
    "    )\n",
    "\n",
    "    y_pred = bst.predict(dtest)\n",
    "    rmse_test = root_mean_squared_error(y_test,y_pred)\n",
    "    trial.set_user_attr('iter',model.best_iteration)\n",
    "\n",
    "    return rmse_test\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    sampler = TPESampler()\n",
    "    study = optuna.create_study(\n",
    "                                sampler = sampler,\n",
    "                                storage=optuna_db_path,\n",
    "                                study_name='SRF_XGBoost_UPdate',\n",
    "                                load_if_exists=True,\n",
    "                                direction=\"minimize\")\n",
    "\n",
    "    study.optimize(objective, n_trials=20000)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain = dtrain,\n",
    "    num_boost_round=5000,\n",
    "    evals=[(dvalid, 'valid')],\n",
    "    early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = xgb.train(\n",
    "    params,\n",
    "    dtrain = dreport,\n",
    "    num_boost_round= model.best_iteration\n",
    "    #evals=[(dvalid, 'valid')],\n",
    "    #early_stopping_rounds=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgboost = bst.predict(dtest)\n",
    "rmse_xgboost = root_mean_squared_error(y_test,y_pred_xgboost)\n",
    "print(rmse_xgboost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "#### without pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna.samplers import RandomSampler, TPESampler, GridSampler, CmaEsSampler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "        param = {\n",
    "        \"verbosity\": 1,\n",
    "        \"eval_metric\": 'rmse',\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", ['gbtree','dart']),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 0.5, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0.5, 1.0, log=True),\n",
    "        }\n",
    "\n",
    "\n",
    "        param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 5)\n",
    "        param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "        param[\"gamma\"] = trial.suggest_float(\"gamma\", 0.5, 1.0, log=True)\n",
    "        param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "        \n",
    "        #sample_type = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "        #normalize_type = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "        #rate_drop = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "        #skip_drop = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        \n",
    "        #if param['booster'] == 'dart':\n",
    "        #    param[\"sample_type\"] = sample_type\n",
    "        #    param[\"normalize_type\"] = normalize_type\n",
    "            #param[\"rate_drop\"] = rate_drop\n",
    "            #param[\"skip_drop\"] = skip_drop\n",
    "\n",
    "        model = xgb.train(\n",
    "                param,\n",
    "                dtrain = dtrain,\n",
    "                num_boost_round=500,\n",
    "                evals=[(dvalid, 'valid')],\n",
    "                early_stopping_rounds=20\n",
    "            )\n",
    "        \n",
    "        final_model = xgb.train(\n",
    "            param,\n",
    "            dtrain = dreport,\n",
    "            num_boost_round = model.best_iteration\n",
    "        )\n",
    "\n",
    "        y_pred = final_model.predict(dtest)\n",
    "        rmse_trial = root_mean_squared_error(y_test,y_pred)\n",
    "        trial.set_user_attr('iter',model.best_iteration)\n",
    "\n",
    "        return rmse_trial\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sampler = TPESampler()\n",
    "    study = optuna.create_study(\n",
    "                                sampler = sampler,\n",
    "                                storage=optuna_db_path,\n",
    "                                study_name='SRF_xgboost_update_new_version',\n",
    "                                load_if_exists=True,\n",
    "                                direction=\"minimize\")\n",
    "\n",
    "    study.optimize(objective, n_trials=20000)\n",
    "\n",
    "    print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: {}\".format(trial.value))\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    #try:\n",
    "        param = {\n",
    "        \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        \"tree_method\": \"exact\",\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-3, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-3, 1.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "        if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "            param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            param[\"eta\"] = trial.suggest_float(\"eta\", 1e-4, 1.0, log=True)\n",
    "            param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-4, 1.0, log=True)\n",
    "            param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    \n",
    "        if param[\"booster\"] == \"dart\":\n",
    "            param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-3, 1.0, log=True)\n",
    "            param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-3, 1.0, log=True)\n",
    "\n",
    "        boost = XGBRegressor(**param,  n_jobs=-1)\n",
    "\n",
    "        score = np.mean((boost.fit(X_train.values, y_train).predict(X_test.values)-y_test)**2)\n",
    "        return (score)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sampler = optuna.samplers.TPESampler(\n",
    "        #multivariate = True,\n",
    "        #group = True\n",
    "    )\n",
    "    study = optuna.create_study(direction='minimize', \n",
    "                                storage=optuna_db_path, \n",
    "                                study_name='boost_complete_run_3', \n",
    "                                sampler = sampler,\n",
    "                                load_if_exists=True)\n",
    "    study.optimize(objective,\n",
    "                   n_trials= 1000)\n",
    "    \n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "\"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "\n",
    "\"tree_method\": \"exact\",\n",
    "\n",
    "\"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "\n",
    "\"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "\n",
    "\"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "\n",
    "\"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "\n",
    "\"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "boost = XGBRegressor().fit(X_train.values, y_train.values)\n",
    "test_mse = np.mean((boost.predict(X_test) - y_test)**2)\n",
    "test_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boost_opt = XGBRegressor(**trial.params).fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mse = np.mean((Boost_opt.predict(X_test) - y_test)**2)\n",
    "test_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "### with pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train, test_size = 0.3)\n",
    "valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_db_path = \"sqlite:////Users/mingyanggao/Desktop/SRF/my_optuna_study.db\"\n",
    "\n",
    "def objective(trial):\n",
    "        #max_seconds = 180\n",
    "        param = {\n",
    "        \"learning_rate\": trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        \"tree_method\": \"exact\",\n",
    "        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n",
    "        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n",
    "    }\n",
    "\n",
    "        if param[\"booster\"] in [\"gbtree\", \"dart\"]:\n",
    "            param[\"max_depth\"] = trial.suggest_int(\"max_depth\", 3, 9, step=2)\n",
    "            param[\"min_child_weight\"] = trial.suggest_int(\"min_child_weight\", 2, 10)\n",
    "            param[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "            param[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            param[\"grow_policy\"] = trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"])\n",
    "    \n",
    "        if param[\"booster\"] == \"dart\":\n",
    "            param[\"sample_type\"] = trial.suggest_categorical(\"sample_type\", [\"uniform\", \"weighted\"])\n",
    "            param[\"normalize_type\"] = trial.suggest_categorical(\"normalize_type\", [\"tree\", \"forest\"])\n",
    "            param[\"rate_drop\"] = trial.suggest_float(\"rate_drop\", 1e-8, 1.0, log=True)\n",
    "            param[\"skip_drop\"] = trial.suggest_float(\"skip_drop\", 1e-8, 1.0, log=True)\n",
    "        \n",
    "        boost = boost = XGBRegressor(**param)\n",
    "\n",
    "        #pruning\n",
    "        #for step in range(50):\n",
    "            #start_time = time.time()\n",
    "\n",
    "        #    train_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train, test_size = 0.3)\n",
    "        #    boost.fit(train_X.values, train_y.values)\n",
    "\n",
    "            #if time.time() - start_time > max_seconds:\n",
    "            #    raise optuna.TrialPruned()\n",
    "\n",
    "        #    intermediate_value = np.mean((boost.predict(valid_X.values) - valid_y.values)**2)\n",
    "            \n",
    "        #    trial.report(intermediate_value,step)\n",
    "        #    if trial.should_prune():\n",
    "        #        raise optuna.TrialPruned()\n",
    "        \n",
    "        #if not pruned\n",
    "        scores = skm.cross_val_score(boost, X_train.values, y_train.values, n_jobs=-1, cv=3, scoring='neg_mean_squared_error')\n",
    "        actual_mse_scores = [-s for s in scores]\n",
    "        mean_mse = np.mean(actual_mse_scores)\n",
    "        report_cross_validation_scores(trial, scores.tolist())\n",
    "        return (mean_mse)\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(direction='minimize', \n",
    "                                storage=optuna_db_path, \n",
    "                                study_name='boost_official_5%', \n",
    "                                load_if_exists=True)\n",
    "    study.optimize(objective,\n",
    "                   n_trials= 200)\n",
    "    \n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_best = {\n",
    "    'learning_rate': 0.1453396227151677,\n",
    "    'booster': 'dart',\n",
    "    'lambda': 3.8703489993224236e-08,\n",
    "    'alpha': 0.003032430517597912,\n",
    "    'subsample': 0.790961129756034,\n",
    "    'colsample_bytree': 0.7273439201999695,\n",
    "    'max_depth': 7,\n",
    "    'min_child_weight': 8,\n",
    "    'eta': 1.7488976584656106e-07,\n",
    "    'gamma': 0.031707710814832854,\n",
    "    'grow_policy': 'lossguide',\n",
    "    'sample_type': 'uniform',\n",
    "    'normalize_type': 'forest',\n",
    "    'rate_drop': 6.726506333721989e-08,\n",
    "    'skip_drop': 0.001512580988907405\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "best = XGBRegressor(**params_best)\n",
    "best.fit(X_train.values, y_train.values)\n",
    "x = np.mean((best.predict(X_test.values)-y_test.values)**2)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 300, step=50)\n",
    "    min_samples_split = trial.suggest_int('min_samples_split', 2, 50, step=1)\n",
    "\n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators = n_estimators,\n",
    "        min_samples_split = min_samples_split,\n",
    "        n_jobs = -1\n",
    "    )\n",
    "\n",
    "    #for step in range(50):\n",
    "    #    train_X, valid_X, train_y, valid_y = train_test_split(X_train, y_train, test_size = 0.3)\n",
    "    #    rf.fit(train_X.values, train_y.values)\n",
    "    #    intermediate_value = np.mean((rf.predict(valid_X.values) - valid_y.values)**2)\n",
    "    #    trial.report(intermediate_value,step)\n",
    "    #    if trial.should_prune():\n",
    "    #        raise optuna.TrialPruned()\n",
    "\n",
    "    scores = skm.cross_val_score(rf, X_train, y_train, n_jobs=-1, cv=3, scoring='neg_mean_squared_error')\n",
    "    actual_mse_scores = [-s for s in scores]\n",
    "    mean_mse = np.mean(actual_mse_scores)\n",
    "    report_cross_validation_scores(trial, scores.tolist())\n",
    "    return (mean_mse)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(direction='minimize', \n",
    "                                storage=optuna_db_path, \n",
    "                                study_name='rf_complete',  # Changed study name to avoid distribution conflict\n",
    "                                load_if_exists=True)\n",
    "    study.optimize(objective,\n",
    "                   n_trials= 20)\n",
    "    \n",
    "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "    print(\"Study statistics: \")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "    print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "    print(\"Best trial:\")\n",
    "    trial = study.best_trial\n",
    "\n",
    "    print(\"  Value: \", trial.value)\n",
    "\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial.params.items():\n",
    "        print(\"    {}: {}\".format(key, value))\n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "## FC NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "### Torch with no Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "DEVICE = torch.device('mps')\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = X_train.shape[1]\n",
    "X_train_numeric = X_train.astype(np.float32)\n",
    "X_test_numeric = X_test.astype(np.float32)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_numeric.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_numeric.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = X_train_tensor.to(DEVICE)\n",
    "y_train_tensor = y_train_tensor.to(DEVICE)\n",
    "X_test_tensor = X_test_tensor.to(DEVICE)\n",
    "y_test_tensor = y_test_tensor.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FC, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(a, 64),   \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)    # Output: single value (price)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = FC()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(NN.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    NN.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = NN(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch: {epoch}; Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = NN(X_test_tensor)\n",
    "    test_loss = criterion(predictions, y_test_tensor)\n",
    "    print(f\"Test MSE: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97",
   "metadata": {},
   "source": [
    "### Torch with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "def define_model(trial):\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 10, step = 1)\n",
    "    layers = []\n",
    "\n",
    "    in_features = a\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_float(\"dropout_{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "\n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def train_NN(model, optimizer, X_train_NN, y_train_NN):\n",
    "    criterion = nn.MSELoss()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_NN)\n",
    "    loss = criterion(output, y_train_NN)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "def eval_NN(model, X_test_NN, y_test_NN):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_NN)\n",
    "        test_loss = criterion(predictions, y_test_NN)\n",
    "        return test_loss\n",
    "\n",
    "def objective(trial):\n",
    "    X_train_NN = X_train_tensor\n",
    "    y_train_NN = y_train_tensor\n",
    "    X_test_NN = X_test_tensor\n",
    "    y_test_NN = y_test_tensor\n",
    "\n",
    "    model = define_model(trial).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True))\n",
    "\n",
    "    for epoch in range(50):\n",
    "        train_NN(model, optimizer, X_train_NN, y_train_NN)\n",
    "    MSE = eval_NN(model, X_test_NN, y_test_NN)\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTORCH_MPS_HIGH_WATERMARK_RATIO=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(directions= [\"minimize\"],\n",
    "                            storage=optuna_db_path, \n",
    "                            study_name='NN_complete_run', \n",
    "                            load_if_exists=True)\n",
    "                            \n",
    "study.optimize(objective,\n",
    "               n_trials=20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "class best_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(best_NN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(a, 72),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.41393624755510494),\n",
    "            nn.Linear(72, 62),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.30910461566591313),\n",
    "            nn.Linear(62, 36),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.326896643945087),\n",
    "            nn.Linear(36, 1)                      \n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "NN = best_NN().to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(NN.parameters(), lr=0.05046040510249565)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    NN.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = NN(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "NN.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = NN(X_test_tensor)\n",
    "    test_loss = criterion(predictions, y_test_tensor)\n",
    "    print(f\"Test MSE: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "## Best Torch updated by Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "class best_NN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(best_NN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(a, 73),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3120482244670788),\n",
    "            nn.Linear(73, 99),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3480728906394902),\n",
    "            nn.Linear(99, 85),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.249992788940722),\n",
    "            nn.Linear(85, 14),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.22961914614037873),\n",
    "            nn.Linear(14,1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "NN = best_NN().to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(NN.parameters(), lr=0.01129180147251508)\n",
    "\n",
    "for epoch in range(200):\n",
    "    NN.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = NN(X_train_tensor)\n",
    "    loss = criterion(output, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "NN.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = NN(X_test_tensor)\n",
    "    test_loss = criterion(predictions, y_test_tensor)\n",
    "    print(f\"Test MSE: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110",
   "metadata": {},
   "source": [
    "bagging with XGBoost, the actual run will use 100 estimator while in the test only 10 will be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params =     {'learning_rate': 0.11904245065065563,\n",
    "                    'booster': 'gbtree',\n",
    "                    'lambda': 4.572570120030754e-07,\n",
    "                    'alpha': 0.4604968728612804,\n",
    "                    'subsample': 0.9216847013051581,\n",
    "                    'colsample_bytree': 0.7745972825238412,\n",
    "                    'max_depth': 9,\n",
    "                    'min_child_weight': 5,\n",
    "                    'eta': 4.1884110093375496e-07,\n",
    "                    'gamma': 1.3519060667913771e-08,\n",
    "                    'grow_policy': 'lossguide'}\n",
    "Boost_opt = XGBRegressor(**opt_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_XGBoost = BaggingRegressor(\n",
    "    Boost_opt,\n",
    "    n_estimators = 100,\n",
    "    max_samples  = 0.8,\n",
    "    bootstrap = True,\n",
    "    random_state = 1,\n",
    "    oob_score = True,\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "bagging_XGBoost.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_predict_bagging_boosting  = np.mean((bagging_XGBoost.predict(X_test.values) - y_test)**2)\n",
    "MSE_predict_bagging_boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "## Voting/Averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetRegressor\n",
    "net = NeuralNetRegressor(\n",
    "    module = NN,\n",
    "    max_epochs=100,\n",
    "    criterion = nn.MSELoss(),\n",
    "    optimizer = torch.optim.Adam,\n",
    "    lr=0.05046040510249565\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting = VotingRegressor(\n",
    "    estimators = [\n",
    "        ('Boosting', Boost_opt),\n",
    "        ('Net', net)],\n",
    "    verbose = 2)\n",
    "    \n",
    "voting = voting.fit(X_train_tensor.cpu().numpy(), y_train_tensor.cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = X_test_tensor.cpu().numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_predict_voting = np.mean((voting.predict(X_test_tensor.cpu().numpy()) - y_test)**2)\n",
    "MSE_predict_voting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120",
   "metadata": {},
   "source": [
    "# Undone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "using the default single layer for final estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked = [\n",
    "    ('Boosting', XGBoost_opt),\n",
    "    ('DTR', DTR_opt),\n",
    "    ('NN',NN_opt)\n",
    "]\n",
    "\n",
    "stack = StackingRegressor(\n",
    "    estimators = stacked,\n",
    "    final_estimator = None,\n",
    "    cv = 5,\n",
    "    verbose = 2\n",
    ")\n",
    "\n",
    "stack.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_predict_stacking       = np.mean((stack.predict(X_test.values) - y_test)**2)\n",
    "MSE_predict_stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125",
   "metadata": {},
   "source": [
    "using a voting regressor as a customized final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_final = VotingRegressor(\n",
    "    estimators = [\n",
    "        ('Ridge',RidgeCV()),\n",
    "        ('RF', RandomForestRegressor())],\n",
    "    verbose = 2\n",
    "        )\n",
    "\n",
    "stack_vote = StackingRegressor(\n",
    "    estimators = stacked,\n",
    "    final_estimator = alt_final,\n",
    "    cv = 5,\n",
    "    verbose = 2\n",
    "    )\n",
    "\n",
    "stack_vote.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_predict_stack_vote       = np.mean((stack_vote.predict(X_test.values) - y_test)**2)\n",
    "MSE_predict_stack_vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "## SHAP: variable importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "## Neural Network + Kenel SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131",
   "metadata": {},
   "source": [
    "### Forceplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "since the train set is small enough, using only k = 2\n",
    "\n",
    "in the actual run k = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "background = shap.kmeans(X_train, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134",
   "metadata": {},
   "source": [
    "in the trail run the test set is small enough, so producing SHAP values for all\n",
    "\n",
    "in the actual run, would only need a portion of test set's SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# background = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_explainer = shap.KernelExplainer(NN_opt.predict, background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = NN_explainer.shap_values(X_test.iloc[0:1000], verbose =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "  NN_explainer.expected_value, \n",
    "  shap_values[0:1000], \n",
    "  X_test.iloc[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "  NN_explainer.expected_value, \n",
    "  shap_values[0:10], \n",
    "  X_test.iloc[0:10]\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140",
   "metadata": {},
   "source": [
    "### Beehive Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_SHAP_summary_plt = shap.summary_plot(shap_values[0:1000], X_test[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "### Dependence plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "Again in the trail run all data in X_test is used\n",
    "\n",
    "In the actual run only a portion is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list(X_test.columns):\n",
    "    shap.dependence_plot(f , shap_values[0:1000], X_test[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145",
   "metadata": {},
   "source": [
    "## XGBoost + Tree SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146",
   "metadata": {},
   "source": [
    "### Forceplot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147",
   "metadata": {},
   "source": [
    "XGBoost does not support K-means background\n",
    "\n",
    "selecting a portion to predict as calibration\n",
    "\n",
    "using all of X_train in trial run\n",
    "\n",
    "using 50 is enough in the actual run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_boosting = X_train.iloc[0:500].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "Boosting_explainer = shap.TreeExplainer(XGBoost_opt, background_boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "trial: actual\n",
    "\n",
    "actual: all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_boosting = Boosting_explainer.shap_values(X_test.iloc[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_boosting = Boosting_explainer.shap_values(X_test.iloc[0:1000])\n",
    "shap.force_plot(Boosting_explainer.expected_value, shap_values_boosting[0:1000], X_test.iloc[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "### Beehive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_SHAP_summary_plt = shap.summary_plot(shap_values_boosting[0:1000], X_test[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "### Dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list(X_test.columns):\n",
    "    shap.dependence_plot(f, shap_values_boosting[0:1000], X_test[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "## DTR + Tree SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {},
   "source": [
    "### Forceplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_DTR = X_train.iloc[0:50].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR_explainer = shap.TreeExplainer(DTR_opt,background_DTR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values_DTR = DTR_explainer.shap_values(X_test.iloc[0:1000])\n",
    "shap.force_plot(DTR_explainer.expected_value, shap_values_DTR[0:1000], X_test.iloc[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162",
   "metadata": {},
   "source": [
    "### Beehive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_DTR[0:1000], X_test.iloc[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "### Dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list(X_test.columns):\n",
    "    shap.dependence_plot(f, shap_values_DTR[0:1000], X_test.iloc[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "## NN Bagging + Kernel SHAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167",
   "metadata": {},
   "source": [
    "### Forceplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagging_NN_explainer   = shap.KernelExplainer(bagging_NN.predict, background)\n",
    "shap_values_bagging_NN = bagging_NN_explainer.shap_values(X_test.iloc[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(bagging_NN_explainer.expected_value, shap_values_bagging_NN[0:1000], X_test.iloc[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "### Beehive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values_bagging_NN[0:1000], X_test[0:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "### Dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in list(X_test.columns):\n",
    "    shap.dependence_plot(f, shap_values_bagging_NN[0:1000], X_test.iloc[0:1000])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
